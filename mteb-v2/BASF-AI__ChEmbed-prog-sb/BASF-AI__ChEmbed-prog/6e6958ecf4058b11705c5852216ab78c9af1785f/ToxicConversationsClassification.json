{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.661621,
        "f1": 0.508381,
        "f1_weighted": 0.736364,
        "ap": 0.122168,
        "ap_weighted": 0.122168,
        "scores_per_experiment": [
          {
            "accuracy": 0.710449,
            "f1": 0.54072,
            "f1_weighted": 0.775751,
            "ap": 0.134003,
            "ap_weighted": 0.134003
          },
          {
            "accuracy": 0.692383,
            "f1": 0.521447,
            "f1_weighted": 0.76221,
            "ap": 0.119473,
            "ap_weighted": 0.119473
          },
          {
            "accuracy": 0.731445,
            "f1": 0.552072,
            "f1_weighted": 0.790683,
            "ap": 0.136513,
            "ap_weighted": 0.136513
          },
          {
            "accuracy": 0.731934,
            "f1": 0.555174,
            "f1_weighted": 0.791219,
            "ap": 0.140531,
            "ap_weighted": 0.140531
          },
          {
            "accuracy": 0.515137,
            "f1": 0.423904,
            "f1_weighted": 0.616892,
            "ap": 0.103504,
            "ap_weighted": 0.103504
          },
          {
            "accuracy": 0.541016,
            "f1": 0.442198,
            "f1_weighted": 0.639833,
            "ap": 0.110243,
            "ap_weighted": 0.110243
          },
          {
            "accuracy": 0.75293,
            "f1": 0.545258,
            "f1_weighted": 0.803947,
            "ap": 0.11727,
            "ap_weighted": 0.11727
          },
          {
            "accuracy": 0.615234,
            "f1": 0.484283,
            "f1_weighted": 0.703043,
            "ap": 0.117375,
            "ap_weighted": 0.117375
          },
          {
            "accuracy": 0.631348,
            "f1": 0.487504,
            "f1_weighted": 0.716063,
            "ap": 0.111673,
            "ap_weighted": 0.111673
          },
          {
            "accuracy": 0.694336,
            "f1": 0.531251,
            "f1_weighted": 0.763998,
            "ap": 0.131093,
            "ap_weighted": 0.131093
          }
        ],
        "main_score": 0.661621,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 10.186129093170166,
  "kg_co2_emissions": null
}