{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.735609,
        "f1": 0.715435,
        "f1_weighted": 0.734426,
        "scores_per_experiment": [
          {
            "accuracy": 0.743107,
            "f1": 0.726449,
            "f1_weighted": 0.742403
          },
          {
            "accuracy": 0.745124,
            "f1": 0.725935,
            "f1_weighted": 0.746396
          },
          {
            "accuracy": 0.735037,
            "f1": 0.710819,
            "f1_weighted": 0.731996
          },
          {
            "accuracy": 0.744788,
            "f1": 0.719809,
            "f1_weighted": 0.744069
          },
          {
            "accuracy": 0.737391,
            "f1": 0.722706,
            "f1_weighted": 0.733608
          },
          {
            "accuracy": 0.716543,
            "f1": 0.70652,
            "f1_weighted": 0.711918
          },
          {
            "accuracy": 0.731338,
            "f1": 0.709063,
            "f1_weighted": 0.730279
          },
          {
            "accuracy": 0.73302,
            "f1": 0.702976,
            "f1_weighted": 0.732861
          },
          {
            "accuracy": 0.73033,
            "f1": 0.709197,
            "f1_weighted": 0.732884
          },
          {
            "accuracy": 0.739408,
            "f1": 0.720876,
            "f1_weighted": 0.737849
          }
        ],
        "main_score": 0.735609,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 13.671591758728027,
  "kg_co2_emissions": null
}