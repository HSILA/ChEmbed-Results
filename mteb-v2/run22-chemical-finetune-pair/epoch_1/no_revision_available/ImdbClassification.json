{
  "dataset_revision": "3d86128a09e091d6018b6d26cad27f2739fc2db7",
  "task_name": "ImdbClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.864048,
        "f1": 0.86372,
        "f1_weighted": 0.86372,
        "ap": 0.816614,
        "ap_weighted": 0.816614,
        "scores_per_experiment": [
          {
            "accuracy": 0.86388,
            "f1": 0.86319,
            "f1_weighted": 0.86319,
            "ap": 0.836262,
            "ap_weighted": 0.836262
          },
          {
            "accuracy": 0.86716,
            "f1": 0.866851,
            "f1_weighted": 0.866851,
            "ap": 0.832768,
            "ap_weighted": 0.832768
          },
          {
            "accuracy": 0.8592,
            "f1": 0.859069,
            "f1_weighted": 0.859069,
            "ap": 0.801211,
            "ap_weighted": 0.801211
          },
          {
            "accuracy": 0.90076,
            "f1": 0.900573,
            "f1_weighted": 0.900573,
            "ap": 0.876254,
            "ap_weighted": 0.876254
          },
          {
            "accuracy": 0.89388,
            "f1": 0.893862,
            "f1_weighted": 0.893862,
            "ap": 0.848126,
            "ap_weighted": 0.848126
          },
          {
            "accuracy": 0.85464,
            "f1": 0.854523,
            "f1_weighted": 0.854523,
            "ap": 0.796348,
            "ap_weighted": 0.796348
          },
          {
            "accuracy": 0.86948,
            "f1": 0.869366,
            "f1_weighted": 0.869366,
            "ap": 0.813655,
            "ap_weighted": 0.813655
          },
          {
            "accuracy": 0.82276,
            "f1": 0.82127,
            "f1_weighted": 0.82127,
            "ap": 0.749466,
            "ap_weighted": 0.749466
          },
          {
            "accuracy": 0.85744,
            "f1": 0.857416,
            "f1_weighted": 0.857416,
            "ap": 0.803275,
            "ap_weighted": 0.803275
          },
          {
            "accuracy": 0.85128,
            "f1": 0.851081,
            "f1_weighted": 0.851081,
            "ap": 0.808772,
            "ap_weighted": 0.808772
          }
        ],
        "main_score": 0.864048,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 102.95086765289307,
  "kg_co2_emissions": null
}