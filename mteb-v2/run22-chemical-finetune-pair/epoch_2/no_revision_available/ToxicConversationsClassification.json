{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.650586,
        "f1": 0.49866,
        "f1_weighted": 0.727195,
        "ap": 0.116488,
        "ap_weighted": 0.116488,
        "scores_per_experiment": [
          {
            "accuracy": 0.666992,
            "f1": 0.512072,
            "f1_weighted": 0.743512,
            "ap": 0.122095,
            "ap_weighted": 0.122095
          },
          {
            "accuracy": 0.664551,
            "f1": 0.499951,
            "f1_weighted": 0.741457,
            "ap": 0.108781,
            "ap_weighted": 0.108781
          },
          {
            "accuracy": 0.705078,
            "f1": 0.529983,
            "f1_weighted": 0.771474,
            "ap": 0.123059,
            "ap_weighted": 0.123059
          },
          {
            "accuracy": 0.741699,
            "f1": 0.556709,
            "f1_weighted": 0.79777,
            "ap": 0.13655,
            "ap_weighted": 0.13655
          },
          {
            "accuracy": 0.508789,
            "f1": 0.420861,
            "f1_weighted": 0.610821,
            "ap": 0.103977,
            "ap_weighted": 0.103977
          },
          {
            "accuracy": 0.496582,
            "f1": 0.413955,
            "f1_weighted": 0.599195,
            "ap": 0.103527,
            "ap_weighted": 0.103527
          },
          {
            "accuracy": 0.740723,
            "f1": 0.538864,
            "f1_weighted": 0.795694,
            "ap": 0.115822,
            "ap_weighted": 0.115822
          },
          {
            "accuracy": 0.595215,
            "f1": 0.468826,
            "f1_weighted": 0.686938,
            "ap": 0.109597,
            "ap_weighted": 0.109597
          },
          {
            "accuracy": 0.666992,
            "f1": 0.505651,
            "f1_weighted": 0.743388,
            "ap": 0.114049,
            "ap_weighted": 0.114049
          },
          {
            "accuracy": 0.719238,
            "f1": 0.539724,
            "f1_weighted": 0.781696,
            "ap": 0.127422,
            "ap_weighted": 0.127422
          }
        ],
        "main_score": 0.650586,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.113664388656616,
  "kg_co2_emissions": null
}