{
  "dataset_revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a",
  "task_name": "TweetSentimentExtractionClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.596293,
        "f1": 0.59888,
        "f1_weighted": 0.590499,
        "scores_per_experiment": [
          {
            "accuracy": 0.602999,
            "f1": 0.604478,
            "f1_weighted": 0.59358
          },
          {
            "accuracy": 0.589983,
            "f1": 0.592211,
            "f1_weighted": 0.580949
          },
          {
            "accuracy": 0.613752,
            "f1": 0.616709,
            "f1_weighted": 0.610132
          },
          {
            "accuracy": 0.605263,
            "f1": 0.609172,
            "f1_weighted": 0.601022
          },
          {
            "accuracy": 0.615167,
            "f1": 0.618806,
            "f1_weighted": 0.614169
          },
          {
            "accuracy": 0.618846,
            "f1": 0.622525,
            "f1_weighted": 0.615258
          },
          {
            "accuracy": 0.580645,
            "f1": 0.579398,
            "f1_weighted": 0.572197
          },
          {
            "accuracy": 0.603282,
            "f1": 0.605436,
            "f1_weighted": 0.59703
          },
          {
            "accuracy": 0.54867,
            "f1": 0.550397,
            "f1_weighted": 0.539005
          },
          {
            "accuracy": 0.584324,
            "f1": 0.589665,
            "f1_weighted": 0.581643
          }
        ],
        "main_score": 0.596293,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 10.963429927825928,
  "kg_co2_emissions": null
}