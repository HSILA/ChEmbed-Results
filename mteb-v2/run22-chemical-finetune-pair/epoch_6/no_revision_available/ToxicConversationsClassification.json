{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.647559,
        "f1": 0.49848,
        "f1_weighted": 0.72486,
        "ap": 0.118154,
        "ap_weighted": 0.118154,
        "scores_per_experiment": [
          {
            "accuracy": 0.643555,
            "f1": 0.4968,
            "f1_weighted": 0.725557,
            "ap": 0.116264,
            "ap_weighted": 0.116264
          },
          {
            "accuracy": 0.665527,
            "f1": 0.503887,
            "f1_weighted": 0.742268,
            "ap": 0.112743,
            "ap_weighted": 0.112743
          },
          {
            "accuracy": 0.704102,
            "f1": 0.534582,
            "f1_weighted": 0.771032,
            "ap": 0.129559,
            "ap_weighted": 0.129559
          },
          {
            "accuracy": 0.73584,
            "f1": 0.558947,
            "f1_weighted": 0.794077,
            "ap": 0.143376,
            "ap_weighted": 0.143376
          },
          {
            "accuracy": 0.513672,
            "f1": 0.425584,
            "f1_weighted": 0.61494,
            "ap": 0.107043,
            "ap_weighted": 0.107043
          },
          {
            "accuracy": 0.47998,
            "f1": 0.404056,
            "f1_weighted": 0.583117,
            "ap": 0.102427,
            "ap_weighted": 0.102427
          },
          {
            "accuracy": 0.729492,
            "f1": 0.532205,
            "f1_weighted": 0.787937,
            "ap": 0.113714,
            "ap_weighted": 0.113714
          },
          {
            "accuracy": 0.60791,
            "f1": 0.474812,
            "f1_weighted": 0.697374,
            "ap": 0.109514,
            "ap_weighted": 0.109514
          },
          {
            "accuracy": 0.681152,
            "f1": 0.514876,
            "f1_weighted": 0.753959,
            "ap": 0.117544,
            "ap_weighted": 0.117544
          },
          {
            "accuracy": 0.714355,
            "f1": 0.539047,
            "f1_weighted": 0.778344,
            "ap": 0.129356,
            "ap_weighted": 0.129356
          }
        ],
        "main_score": 0.647559,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.595437049865723,
  "kg_co2_emissions": null
}