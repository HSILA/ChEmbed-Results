{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.72189,
        "f1": 0.711721,
        "f1_weighted": 0.718555,
        "scores_per_experiment": [
          {
            "accuracy": 0.731002,
            "f1": 0.724862,
            "f1_weighted": 0.726517
          },
          {
            "accuracy": 0.747814,
            "f1": 0.734075,
            "f1_weighted": 0.742553
          },
          {
            "accuracy": 0.714526,
            "f1": 0.705965,
            "f1_weighted": 0.710496
          },
          {
            "accuracy": 0.716207,
            "f1": 0.706352,
            "f1_weighted": 0.714841
          },
          {
            "accuracy": 0.742098,
            "f1": 0.722637,
            "f1_weighted": 0.739297
          },
          {
            "accuracy": 0.721923,
            "f1": 0.712786,
            "f1_weighted": 0.713213
          },
          {
            "accuracy": 0.720915,
            "f1": 0.709565,
            "f1_weighted": 0.720089
          },
          {
            "accuracy": 0.723268,
            "f1": 0.708821,
            "f1_weighted": 0.723455
          },
          {
            "accuracy": 0.706792,
            "f1": 0.701605,
            "f1_weighted": 0.704218
          },
          {
            "accuracy": 0.694351,
            "f1": 0.690538,
            "f1_weighted": 0.690869
          }
        ],
        "main_score": 0.72189,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 5.362253665924072,
  "kg_co2_emissions": null
}