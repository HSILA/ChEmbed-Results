{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.722327,
        "f1": 0.70085,
        "f1_weighted": 0.722026,
        "scores_per_experiment": [
          {
            "accuracy": 0.735037,
            "f1": 0.720715,
            "f1_weighted": 0.734233
          },
          {
            "accuracy": 0.73033,
            "f1": 0.706622,
            "f1_weighted": 0.732639
          },
          {
            "accuracy": 0.724277,
            "f1": 0.691291,
            "f1_weighted": 0.721847
          },
          {
            "accuracy": 0.737727,
            "f1": 0.712553,
            "f1_weighted": 0.738396
          },
          {
            "accuracy": 0.71688,
            "f1": 0.70028,
            "f1_weighted": 0.714662
          },
          {
            "accuracy": 0.704438,
            "f1": 0.68955,
            "f1_weighted": 0.702263
          },
          {
            "accuracy": 0.713181,
            "f1": 0.699133,
            "f1_weighted": 0.712982
          },
          {
            "accuracy": 0.718561,
            "f1": 0.692686,
            "f1_weighted": 0.719298
          },
          {
            "accuracy": 0.717552,
            "f1": 0.693151,
            "f1_weighted": 0.720516
          },
          {
            "accuracy": 0.725286,
            "f1": 0.702521,
            "f1_weighted": 0.72342
          }
        ],
        "main_score": 0.722327,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 18.442760944366455,
  "kg_co2_emissions": null
}