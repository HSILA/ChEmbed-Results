{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_models = [x for x in os.listdir('ChemRxivRetrieval') if not x.startswith('run')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_sources = [\n",
    "    'bedrock__cohere-embed-english-v3',\n",
    "    'bedrock__amazon-titan-embed-text-v1',\n",
    "    'bedrock__amazon-titan-embed-text-v2',\n",
    "    'bedrock__cohere-embed-multilingual-v3',\n",
    "    'openai__text-embedding-3-large',\n",
    "    'openai__text-embedding-ada-002',\n",
    "    'openai__text-embedding-3-small',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_meta(base_dir, model_name):\n",
    "    rev = os.path.join(base_dir, model_name, model_name)\n",
    "    rev = os.listdir(rev)[0]\n",
    "    json_path = os.path.join(base_dir, model_name, model_name, rev, 'model_meta.json')\n",
    "    with open(json_path, 'r') as f:\n",
    "        meta = json.load(f)\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'nomic-ai__nomic-embed-text-v1',\n",
       " 'revision': 'no_revision_available',\n",
       " 'release_date': None,\n",
       " 'languages': ['eng-Latn'],\n",
       " 'n_parameters': None,\n",
       " 'memory_usage_mb': None,\n",
       " 'max_tokens': 8192.0,\n",
       " 'embed_dim': 768,\n",
       " 'license': 'cc-by-nc-4.0',\n",
       " 'open_weights': True,\n",
       " 'public_training_code': None,\n",
       " 'public_training_data': None,\n",
       " 'framework': ['Sentence Transformers', 'PyTorch'],\n",
       " 'reference': None,\n",
       " 'similarity_fn_name': 'cosine',\n",
       " 'use_instructions': True,\n",
       " 'training_datasets': None,\n",
       " 'adapted_from': None,\n",
       " 'superseded_by': None,\n",
       " 'is_cross_encoder': None,\n",
       " 'modalities': ['text'],\n",
       " 'loader': 'BiEncoderWrapper'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_model_meta('ChemRxivRetrieval', 'nomic-ai__nomic-embed-text-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find models without embedding dims present in their model_meta.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_embed_dim_models = []\n",
    "for model in other_models:\n",
    "    meta = read_model_meta('ChemRxivRetrieval', model)\n",
    "    if meta['embed_dim'] is None:\n",
    "        no_embed_dim_models.append(model)\n",
    "        print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_dim(model_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Loads a Hugging Face model and returns its embedding dimension, with GPU support\n",
    "    and special handling for certain Sentence-Transformers repos.\n",
    "\n",
    "    1. Replaces '__' with '/' in the model name.\n",
    "    2. If the base name matches a known Sentence-Transformer, prefix with 'sentence-transformers/'.\n",
    "    3. Attempts to load the corresponding tokenizer; falls back to 'bert-base-uncased' if unavailable.\n",
    "    4. Loads the model (using GPU if available).\n",
    "       - For repos requiring remote code (e.g. nomic-ai), sets trust_remote_code=True.\n",
    "    5. Tokenizes a dummy string and runs a forward pass on the correct device.\n",
    "    6. Returns the hidden-state size (embedding dimension) as an integer.\n",
    "    \"\"\"\n",
    "    SENTENCE_TRANSFORMER_MODELS = {\n",
    "        \"all-MiniLM-L6-v2\",\n",
    "        \"all-mpnet-base-v2\",\n",
    "        \"multi-qa-mpnet-base-dot-v1\",\n",
    "        \"all-MiniLM-L12-v2\",\n",
    "    }\n",
    "\n",
    "    # Normalize repository path\n",
    "    repo = model_name.replace(\"__\", \"/\")\n",
    "    base = repo.split(\"/\")[-1]\n",
    "\n",
    "    # Handle Sentence-Transformer shortcuts\n",
    "    if base in SENTENCE_TRANSFORMER_MODELS:\n",
    "        repo = f\"sentence-transformers/{base}\"\n",
    "\n",
    "    # Detect device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load tokenizer with fallback\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(repo)\n",
    "    except Exception:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Prepare dummy input\n",
    "    inputs = tokenizer(\"This is a dummy string\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Load model, with remote code trust if needed\n",
    "    trust_remote = repo.startswith(\"nomic-ai/\")\n",
    "    model = AutoModel.from_pretrained(repo, trust_remote_code=trust_remote).to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Extract embedding dimension\n",
    "    embed_dim = outputs.last_hidden_state.shape[-1]\n",
    "    return int(embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in no_embed_dim_models:\n",
    "    try:\n",
    "        dim = get_embed_dim(model)\n",
    "        print(f\"Embed dimension for '{model}': {dim}\")\n",
    "        print(30 * '*')\n",
    "    except Exception as e:\n",
    "        print(f\"Error for '{model}': {e}\")\n",
    "        print(30 * '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim_updated = {\n",
    "  \"answerdotai__ModernBERT-large\": 1024,\n",
    "  \"BAAI__bge-large-en-v1.5\": 1024,\n",
    "  \"BAAI__bge-base-en-v1.5\": 768,\n",
    "  \"all-MiniLM-L6-v2\": 384,\n",
    "  \"all-mpnet-base-v2\": 768,\n",
    "  \"multi-qa-mpnet-base-dot-v1\": 768,\n",
    "  \"nomic-ai__nomic-embed-text-v2-moe\": 768,\n",
    "  \"answerdotai__ModernBERT-base\": 768,\n",
    "  \"BAAI__bge-small-en\": 384,\n",
    "  \"all-MiniLM-L12-v2\": 384,\n",
    "  \"nomic-ai__nomic-bert-2048\": 768,\n",
    "  \"BAAI__bge-base-en\": 768,\n",
    "  \"recobo__chemical-bert-uncased\": 768,\n",
    "  \"google-bert__bert-base-uncased\": 768,\n",
    "  \"BAAI__bge-large-en\": 1024,\n",
    "  \"allenai__scibert_scivocab_uncased\": 768,\n",
    "  \"BAAI__bge-small-en-v1.5\": 384,\n",
    "  'm3rg-iitd__matscibert': 768\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(embed_dim_updated) == len(no_embed_dim_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_meta(base_dir, model_name, key_to_update, new_value):\n",
    "    \"\"\"\n",
    "    Reads the model_meta.json for the given model,\n",
    "    updates a specific key with new_value,\n",
    "    and writes the updated JSON back without modifying other entries.\n",
    "    \"\"\"\n",
    "    # Determine the revision directory\n",
    "    model_root = os.path.join(base_dir, model_name, model_name)\n",
    "    rev = os.listdir(model_root)[0]\n",
    "    json_path = os.path.join(model_root, rev, 'model_meta.json')\n",
    "\n",
    "    # Read existing metadata\n",
    "    with open(json_path, 'r') as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    # Update the specified key\n",
    "    meta[key_to_update] = new_value\n",
    "\n",
    "    # Write it back\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\"Updated '{key_to_update}' to '{new_value}' in {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 'embed_dim' to '1024' in ChemRxivRetrieval/answerdotai__ModernBERT-large/answerdotai__ModernBERT-large/45bb4654a4d5aaff24dd11d4781fa46d39bf8c13/model_meta.json\n",
      "Updated 'embed_dim' to '1024' in ChemRxivRetrieval/BAAI__bge-large-en-v1.5/BAAI__bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/model_meta.json\n",
      "Updated 'embed_dim' to '768' in ChemRxivRetrieval/BAAI__bge-base-en-v1.5/BAAI__bge-base-en-v1.5/a5beb1e3e68b9ab74eb54cfd186867f64f240e1a/model_meta.json\n",
      "Updated 'embed_dim' to '384' in ChemRxivRetrieval/all-MiniLM-L6-v2/all-MiniLM-L6-v2/8b3219a92973c328a8e22fadcfa821b5dc75636a/model_meta.json\n",
      "Updated 'embed_dim' to '768' in ChemRxivRetrieval/all-mpnet-base-v2/all-mpnet-base-v2/84f2bcc00d77236f9e89c8a360a00fb1139bf47d/model_meta.json\n",
      "Updated 'embed_dim' to '768' in ChemRxivRetrieval/multi-qa-mpnet-base-dot-v1/multi-qa-mpnet-base-dot-v1/3af7c6da5b3e1bea796ef6c97fe237538cbe6e7f/model_meta.json\n",
      "Updated 'embed_dim' to '768' in ChemRxivRetrieval/nomic-ai__nomic-embed-text-v2-moe/nomic-ai__nomic-embed-text-v2-moe/1066b6599d099fbb93dfcb64f9c37a7c9e503e85/model_meta.json\n",
      "Updated 'embed_dim' to '768' in ChemRxivRetrieval/answerdotai__ModernBERT-base/answerdotai__ModernBERT-base/8949b909ec900327062f0ebf497f51aef5e6f0c8/model_meta.json\n",
      "Updated 'embed_dim' to '384' in ChemRxivRetrieval/BAAI__bge-small-en/BAAI__bge-small-en/2275a7bdee235e9b4f01fa73aa60d3311983cfea/model_meta.json\n",
      "Updated 'embed_dim' to '384' in ChemRxivRetrieval/all-MiniLM-L12-v2/all-MiniLM-L12-v2/a05860a77cef7b37e0048a7864658139bc18a854/model_meta.json\n",
      "Updated 'embed_dim' to '768' in ChemRxivRetrieval/nomic-ai__nomic-bert-2048/nomic-ai__nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/model_meta.json\n",
      "Updated 'embed_dim' to '768' in ChemRxivRetrieval/BAAI__bge-base-en/BAAI__bge-base-en/b737bf5dcc6ee8bdc530531266b4804a5d77b5d8/model_meta.json\n",
      "Updated 'embed_dim' to '768' in ChemRxivRetrieval/recobo__chemical-bert-uncased/recobo__chemical-bert-uncased/498698d28fcf7ce5954852a0444c864bdf232b64/model_meta.json\n",
      "Updated 'embed_dim' to '768' in ChemRxivRetrieval/google-bert__bert-base-uncased/google-bert__bert-base-uncased/86b5e0934494bd15c9632b12f734a8a67f723594/model_meta.json\n",
      "Updated 'embed_dim' to '1024' in ChemRxivRetrieval/BAAI__bge-large-en/BAAI__bge-large-en/abe7d9d814b775ca171121fb03f394dc42974275/model_meta.json\n",
      "Updated 'embed_dim' to '768' in ChemRxivRetrieval/allenai__scibert_scivocab_uncased/allenai__scibert_scivocab_uncased/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/model_meta.json\n",
      "Updated 'embed_dim' to '384' in ChemRxivRetrieval/BAAI__bge-small-en-v1.5/BAAI__bge-small-en-v1.5/5c38ec7c405ec4b44b94cc5a9bb96e735b38267a/model_meta.json\n",
      "Updated 'embed_dim' to '768' in ChemRxivRetrieval/m3rg-iitd__matscibert/m3rg-iitd__matscibert/ced9d8f5f208712c4a90f98a246fe32155b29995/model_meta.json\n"
     ]
    }
   ],
   "source": [
    "for model in embed_dim_updated:\n",
    "    update_model_meta('ChemRxivRetrieval', model, 'embed_dim', embed_dim_updated[model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Num Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m3rg-iitd__matscibert\n",
      "answerdotai__ModernBERT-large\n",
      "BAAI__bge-large-en-v1.5\n",
      "BAAI__bge-base-en-v1.5\n",
      "all-MiniLM-L6-v2\n",
      "all-mpnet-base-v2\n",
      "nomic-ai__nomic-embed-text-v1-unsupervised\n",
      "multi-qa-mpnet-base-dot-v1\n",
      "nomic-ai__nomic-embed-text-v2-moe\n",
      "answerdotai__ModernBERT-base\n",
      "BAAI__bge-small-en\n",
      "nomic-ai__nomic-embed-text-v1\n",
      "all-MiniLM-L12-v2\n",
      "nomic-ai__nomic-bert-2048\n",
      "BAAI__bge-base-en\n",
      "recobo__chemical-bert-uncased\n",
      "google-bert__bert-base-uncased\n",
      "BAAI__bge-large-en\n",
      "allenai__scibert_scivocab_uncased\n",
      "BAAI__bge-small-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "no_n_param_models = []\n",
    "for model in other_models:\n",
    "    meta = read_model_meta('ChemRxivRetrieval', model)\n",
    "    if meta['n_parameters'] is None and model not in closed_sources:\n",
    "        no_n_param_models.append(model)\n",
    "        print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Loads a Hugging Face model and returns its total number of parameters.\n",
    "    \n",
    "    Steps:\n",
    "    1. Replace '__' with '/' in the model identifier.\n",
    "    2. Prefix known Sentence-Transformer models with 'sentence-transformers/'.\n",
    "    3. Detect and use GPU if available.\n",
    "    4. Load the model (trusting remote code for nomic-ai repos).\n",
    "    5. Sum up all parameters.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): The HF repo name, e.g. 'BAAI__bge-large-en'.\n",
    "    \n",
    "    Returns:\n",
    "        int: Total number of parameters (in millions if `in_millions=True`, else exact).\n",
    "    \"\"\"\n",
    "    SENTENCE_TRANSFORMER_MODELS = {\n",
    "        \"all-MiniLM-L6-v2\",\n",
    "        \"all-mpnet-base-v2\",\n",
    "        \"multi-qa-mpnet-base-dot-v1\",\n",
    "        \"all-MiniLM-L12-v2\",\n",
    "    }\n",
    "    # Normalize repo path\n",
    "    repo = model_name.replace(\"__\", \"/\")\n",
    "    base = repo.split(\"/\")[-1]\n",
    "    if base in SENTENCE_TRANSFORMER_MODELS:\n",
    "        repo = f\"sentence-transformers/{base}\"\n",
    "\n",
    "    # Choose device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load model (no tokenizer needed here)\n",
    "    trust_remote = repo.startswith(\"nomic-ai/\")\n",
    "    model = AutoModel.from_pretrained(repo, trust_remote_code=trust_remote).to(device)\n",
    "\n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters for 'm3rg-iitd__matscibert': 109918464\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters for 'answerdotai__ModernBERT-large': 394781696\n",
      "******************************\n",
      "Num parameters for 'BAAI__bge-large-en-v1.5': 335141888\n",
      "******************************\n",
      "Num parameters for 'BAAI__bge-base-en-v1.5': 109482240\n",
      "******************************\n",
      "Num parameters for 'all-MiniLM-L6-v2': 22713216\n",
      "******************************\n",
      "Num parameters for 'all-mpnet-base-v2': 109486464\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters for 'nomic-ai__nomic-embed-text-v1-unsupervised': 136731648\n",
      "******************************\n",
      "Num parameters for 'multi-qa-mpnet-base-dot-v1': 109486464\n",
      "******************************\n",
      "Num parameters for 'nomic-ai__nomic-embed-text-v2-moe': 475292928\n",
      "******************************\n",
      "Num parameters for 'answerdotai__ModernBERT-base': 149014272\n",
      "******************************\n",
      "Num parameters for 'BAAI__bge-small-en': 33360000\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters for 'nomic-ai__nomic-embed-text-v1': 136731648\n",
      "******************************\n",
      "Num parameters for 'all-MiniLM-L12-v2': 33360000\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters for 'nomic-ai__nomic-bert-2048': 136731648\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at recobo/chemical-bert-uncased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters for 'BAAI__bge-base-en': 109482240\n",
      "******************************\n",
      "Num parameters for 'recobo__chemical-bert-uncased': 109918464\n",
      "******************************\n",
      "Num parameters for 'google-bert__bert-base-uncased': 109482240\n",
      "******************************\n",
      "Num parameters for 'BAAI__bge-large-en': 335141888\n",
      "******************************\n",
      "Num parameters for 'allenai__scibert_scivocab_uncased': 109918464\n",
      "******************************\n",
      "Num parameters for 'BAAI__bge-small-en-v1.5': 33360000\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "for model in no_n_param_models:\n",
    "    try:\n",
    "        dim = get_num_parameters(model)\n",
    "        print(f\"Num parameters for '{model}': {dim}\")\n",
    "        print(30 * '*')\n",
    "    except Exception as e:\n",
    "        print(f\"Error for '{model}': {e}\")\n",
    "        print(30 * '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(no_n_param_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params_updated = {\n",
    "    \"m3rg-iitd__matscibert\": 109918464,\n",
    "    \"answerdotai__ModernBERT-large\": 394781696,\n",
    "    \"BAAI__bge-large-en-v1.5\": 335141888,\n",
    "    \"BAAI__bge-base-en-v1.5\": 109482240,\n",
    "    \"all-MiniLM-L6-v2\": 22713216,\n",
    "    \"all-mpnet-base-v2\": 109486464,\n",
    "    \"nomic-ai__nomic-embed-text-v1-unsupervised\": 136731648,\n",
    "    \"multi-qa-mpnet-base-dot-v1\": 109486464,\n",
    "    \"nomic-ai__nomic-embed-text-v2-moe\": 475292928,\n",
    "    \"answerdotai__ModernBERT-base\": 149014272,\n",
    "    \"BAAI__bge-small-en\": 33360000,\n",
    "    \"nomic-ai__nomic-embed-text-v1\": 136731648,\n",
    "    \"all-MiniLM-L12-v2\": 33360000,\n",
    "    \"nomic-ai__nomic-bert-2048\": 136731648,\n",
    "    \"BAAI__bge-base-en\": 109482240,\n",
    "    \"recobo__chemical-bert-uncased\": 109918464,\n",
    "    \"google-bert__bert-base-uncased\": 109482240,\n",
    "    \"BAAI__bge-large-en\": 335141888,\n",
    "    \"allenai__scibert_scivocab_uncased\": 109918464,\n",
    "    \"BAAI__bge-small-en-v1.5\": 33360000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(n_params_updated) == len(no_n_param_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 'n_parameters' to '109918464' in ChemRxivRetrieval/m3rg-iitd__matscibert/m3rg-iitd__matscibert/ced9d8f5f208712c4a90f98a246fe32155b29995/model_meta.json\n",
      "Updated 'n_parameters' to '394781696' in ChemRxivRetrieval/answerdotai__ModernBERT-large/answerdotai__ModernBERT-large/45bb4654a4d5aaff24dd11d4781fa46d39bf8c13/model_meta.json\n",
      "Updated 'n_parameters' to '335141888' in ChemRxivRetrieval/BAAI__bge-large-en-v1.5/BAAI__bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/model_meta.json\n",
      "Updated 'n_parameters' to '109482240' in ChemRxivRetrieval/BAAI__bge-base-en-v1.5/BAAI__bge-base-en-v1.5/a5beb1e3e68b9ab74eb54cfd186867f64f240e1a/model_meta.json\n",
      "Updated 'n_parameters' to '22713216' in ChemRxivRetrieval/all-MiniLM-L6-v2/all-MiniLM-L6-v2/8b3219a92973c328a8e22fadcfa821b5dc75636a/model_meta.json\n",
      "Updated 'n_parameters' to '109486464' in ChemRxivRetrieval/all-mpnet-base-v2/all-mpnet-base-v2/84f2bcc00d77236f9e89c8a360a00fb1139bf47d/model_meta.json\n",
      "Updated 'n_parameters' to '136731648' in ChemRxivRetrieval/nomic-ai__nomic-embed-text-v1-unsupervised/nomic-ai__nomic-embed-text-v1-unsupervised/no_revision_available/model_meta.json\n",
      "Updated 'n_parameters' to '109486464' in ChemRxivRetrieval/multi-qa-mpnet-base-dot-v1/multi-qa-mpnet-base-dot-v1/3af7c6da5b3e1bea796ef6c97fe237538cbe6e7f/model_meta.json\n",
      "Updated 'n_parameters' to '475292928' in ChemRxivRetrieval/nomic-ai__nomic-embed-text-v2-moe/nomic-ai__nomic-embed-text-v2-moe/1066b6599d099fbb93dfcb64f9c37a7c9e503e85/model_meta.json\n",
      "Updated 'n_parameters' to '149014272' in ChemRxivRetrieval/answerdotai__ModernBERT-base/answerdotai__ModernBERT-base/8949b909ec900327062f0ebf497f51aef5e6f0c8/model_meta.json\n",
      "Updated 'n_parameters' to '33360000' in ChemRxivRetrieval/BAAI__bge-small-en/BAAI__bge-small-en/2275a7bdee235e9b4f01fa73aa60d3311983cfea/model_meta.json\n",
      "Updated 'n_parameters' to '136731648' in ChemRxivRetrieval/nomic-ai__nomic-embed-text-v1/nomic-ai__nomic-embed-text-v1/no_revision_available/model_meta.json\n",
      "Updated 'n_parameters' to '33360000' in ChemRxivRetrieval/all-MiniLM-L12-v2/all-MiniLM-L12-v2/a05860a77cef7b37e0048a7864658139bc18a854/model_meta.json\n",
      "Updated 'n_parameters' to '136731648' in ChemRxivRetrieval/nomic-ai__nomic-bert-2048/nomic-ai__nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/model_meta.json\n",
      "Updated 'n_parameters' to '109482240' in ChemRxivRetrieval/BAAI__bge-base-en/BAAI__bge-base-en/b737bf5dcc6ee8bdc530531266b4804a5d77b5d8/model_meta.json\n",
      "Updated 'n_parameters' to '109918464' in ChemRxivRetrieval/recobo__chemical-bert-uncased/recobo__chemical-bert-uncased/498698d28fcf7ce5954852a0444c864bdf232b64/model_meta.json\n",
      "Updated 'n_parameters' to '109482240' in ChemRxivRetrieval/google-bert__bert-base-uncased/google-bert__bert-base-uncased/86b5e0934494bd15c9632b12f734a8a67f723594/model_meta.json\n",
      "Updated 'n_parameters' to '335141888' in ChemRxivRetrieval/BAAI__bge-large-en/BAAI__bge-large-en/abe7d9d814b775ca171121fb03f394dc42974275/model_meta.json\n",
      "Updated 'n_parameters' to '109918464' in ChemRxivRetrieval/allenai__scibert_scivocab_uncased/allenai__scibert_scivocab_uncased/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/model_meta.json\n",
      "Updated 'n_parameters' to '33360000' in ChemRxivRetrieval/BAAI__bge-small-en-v1.5/BAAI__bge-small-en-v1.5/5c38ec7c405ec4b44b94cc5a9bb96e735b38267a/model_meta.json\n"
     ]
    }
   ],
   "source": [
    "for model in n_params_updated:\n",
    "    update_model_meta('ChemRxivRetrieval', model, 'n_parameters', n_params_updated[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
