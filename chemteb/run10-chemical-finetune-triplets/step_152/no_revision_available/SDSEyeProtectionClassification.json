{
  "dataset_revision": "35cbe5ee544dd26e343238a333de4568e6f77819",
  "task_name": "SDSEyeProtectionClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.66765,
        "f1": 0.404611,
        "f1_weighted": 0.795665,
        "ap": 0.998518,
        "ap_weighted": 0.998518,
        "scores_per_experiment": [
          {
            "accuracy": 0.7145,
            "f1": 0.421793,
            "f1_weighted": 0.83113,
            "ap": 0.998286,
            "ap_weighted": 0.998286
          },
          {
            "accuracy": 0.6115,
            "f1": 0.383067,
            "f1_weighted": 0.756594,
            "ap": 0.998028,
            "ap_weighted": 0.998028
          },
          {
            "accuracy": 0.6005,
            "f1": 0.379839,
            "f1_weighted": 0.747915,
            "ap": 0.998499,
            "ap_weighted": 0.998499
          },
          {
            "accuracy": 0.6665,
            "f1": 0.405625,
            "f1_weighted": 0.79743,
            "ap": 0.998665,
            "ap_weighted": 0.998665
          },
          {
            "accuracy": 0.662,
            "f1": 0.403918,
            "f1_weighted": 0.794179,
            "ap": 0.998653,
            "ap_weighted": 0.998653
          },
          {
            "accuracy": 0.788,
            "f1": 0.447593,
            "f1_weighted": 0.879064,
            "ap": 0.99847,
            "ap_weighted": 0.99847
          },
          {
            "accuracy": 0.7065,
            "f1": 0.420526,
            "f1_weighted": 0.825571,
            "ap": 0.998765,
            "ap_weighted": 0.998765
          },
          {
            "accuracy": 0.788,
            "f1": 0.449842,
            "f1_weighted": 0.879009,
            "ap": 0.998969,
            "ap_weighted": 0.998969
          },
          {
            "accuracy": 0.544,
            "f1": 0.356296,
            "f1_weighted": 0.702158,
            "ap": 0.998358,
            "ap_weighted": 0.998358
          },
          {
            "accuracy": 0.595,
            "f1": 0.377612,
            "f1_weighted": 0.743604,
            "ap": 0.998485,
            "ap_weighted": 0.998485
          }
        ],
        "main_score": 0.66765,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 10.844379425048828,
  "kg_co2_emissions": null
}