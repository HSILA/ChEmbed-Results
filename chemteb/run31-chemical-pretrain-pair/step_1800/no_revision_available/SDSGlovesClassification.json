{
  "dataset_revision": "c723236c5ec417d79512e6104aca9d2cd88168f6",
  "task_name": "SDSGlovesClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.57065,
        "f1": 0.358441,
        "f1_weighted": 0.701774,
        "ap": 0.99683,
        "ap_weighted": 0.99683,
        "scores_per_experiment": [
          {
            "accuracy": 0.7155,
            "f1": 0.422152,
            "f1_weighted": 0.830575,
            "ap": 0.996366,
            "ap_weighted": 0.996366
          },
          {
            "accuracy": 0.795,
            "f1": 0.452339,
            "f1_weighted": 0.882073,
            "ap": 0.997182,
            "ap_weighted": 0.997182
          },
          {
            "accuracy": 0.316,
            "f1": 0.243797,
            "f1_weighted": 0.475594,
            "ap": 0.996754,
            "ap_weighted": 0.996754
          },
          {
            "accuracy": 0.6185,
            "f1": 0.388247,
            "f1_weighted": 0.760555,
            "ap": 0.996972,
            "ap_weighted": 0.996972
          },
          {
            "accuracy": 0.7685,
            "f1": 0.438758,
            "f1_weighted": 0.865508,
            "ap": 0.996082,
            "ap_weighted": 0.996082
          },
          {
            "accuracy": 0.7045,
            "f1": 0.419792,
            "f1_weighted": 0.822976,
            "ap": 0.996819,
            "ap_weighted": 0.996819
          },
          {
            "accuracy": 0.5775,
            "f1": 0.371508,
            "f1_weighted": 0.728441,
            "ap": 0.996807,
            "ap_weighted": 0.996807
          },
          {
            "accuracy": 0.2225,
            "f1": 0.184634,
            "f1_weighted": 0.35894,
            "ap": 0.996379,
            "ap_weighted": 0.996379
          },
          {
            "accuracy": 0.3615,
            "f1": 0.270317,
            "f1_weighted": 0.526197,
            "ap": 0.997436,
            "ap_weighted": 0.997436
          },
          {
            "accuracy": 0.627,
            "f1": 0.392863,
            "f1_weighted": 0.766879,
            "ap": 0.997504,
            "ap_weighted": 0.997504
          }
        ],
        "main_score": 0.57065,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 16.00235891342163,
  "kg_co2_emissions": null
}