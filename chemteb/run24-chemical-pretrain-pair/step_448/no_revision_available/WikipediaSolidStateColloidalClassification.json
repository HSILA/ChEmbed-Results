{
  "dataset_revision": "7d8df44e588b6143d4856c781f72f919fa0599a7",
  "task_name": "WikipediaSolidStateColloidalClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.784685,
        "f1": 0.782216,
        "f1_weighted": 0.783256,
        "ap": 0.766018,
        "ap_weighted": 0.766018,
        "scores_per_experiment": [
          {
            "accuracy": 0.804054,
            "f1": 0.804029,
            "f1_weighted": 0.804258,
            "ap": 0.799626,
            "ap_weighted": 0.799626
          },
          {
            "accuracy": 0.617117,
            "f1": 0.615118,
            "f1_weighted": 0.612244,
            "ap": 0.637119,
            "ap_weighted": 0.637119
          },
          {
            "accuracy": 0.804054,
            "f1": 0.80253,
            "f1_weighted": 0.804328,
            "ap": 0.778193,
            "ap_weighted": 0.778193
          },
          {
            "accuracy": 0.835586,
            "f1": 0.830756,
            "f1_weighted": 0.833718,
            "ap": 0.790336,
            "ap_weighted": 0.790336
          },
          {
            "accuracy": 0.725225,
            "f1": 0.724667,
            "f1_weighted": 0.723382,
            "ap": 0.73163,
            "ap_weighted": 0.73163
          },
          {
            "accuracy": 0.842342,
            "f1": 0.841052,
            "f1_weighted": 0.842536,
            "ap": 0.817359,
            "ap_weighted": 0.817359
          },
          {
            "accuracy": 0.826577,
            "f1": 0.820297,
            "f1_weighted": 0.823777,
            "ap": 0.777785,
            "ap_weighted": 0.777785
          },
          {
            "accuracy": 0.810811,
            "f1": 0.809695,
            "f1_weighted": 0.811205,
            "ap": 0.787954,
            "ap_weighted": 0.787954
          },
          {
            "accuracy": 0.774775,
            "f1": 0.774551,
            "f1_weighted": 0.773814,
            "ap": 0.780842,
            "ap_weighted": 0.780842
          },
          {
            "accuracy": 0.806306,
            "f1": 0.799466,
            "f1_weighted": 0.803303,
            "ap": 0.759335,
            "ap_weighted": 0.759335
          }
        ],
        "main_score": 0.784685,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 2.75313138961792,
  "kg_co2_emissions": null
}