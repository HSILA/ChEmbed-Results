{
  "dataset_revision": "35cbe5ee544dd26e343238a333de4568e6f77819",
  "task_name": "SDSEyeProtectionClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.66845,
        "f1": 0.404513,
        "f1_weighted": 0.795114,
        "ap": 0.99852,
        "ap_weighted": 0.99852,
        "scores_per_experiment": [
          {
            "accuracy": 0.698,
            "f1": 0.415832,
            "f1_weighted": 0.819799,
            "ap": 0.998245,
            "ap_weighted": 0.998245
          },
          {
            "accuracy": 0.541,
            "f1": 0.354027,
            "f1_weighted": 0.699823,
            "ap": 0.997851,
            "ap_weighted": 0.997851
          },
          {
            "accuracy": 0.602,
            "f1": 0.380444,
            "f1_weighted": 0.749086,
            "ap": 0.998503,
            "ap_weighted": 0.998503
          },
          {
            "accuracy": 0.716,
            "f1": 0.424001,
            "f1_weighted": 0.832062,
            "ap": 0.998789,
            "ap_weighted": 0.998789
          },
          {
            "accuracy": 0.661,
            "f1": 0.404912,
            "f1_weighted": 0.793338,
            "ap": 0.99915,
            "ap_weighted": 0.99915
          },
          {
            "accuracy": 0.7945,
            "f1": 0.44984,
            "f1_weighted": 0.883114,
            "ap": 0.998486,
            "ap_weighted": 0.998486
          },
          {
            "accuracy": 0.699,
            "f1": 0.416196,
            "f1_weighted": 0.820492,
            "ap": 0.998247,
            "ap_weighted": 0.998247
          },
          {
            "accuracy": 0.817,
            "f1": 0.460226,
            "f1_weighted": 0.896868,
            "ap": 0.999042,
            "ap_weighted": 0.999042
          },
          {
            "accuracy": 0.5185,
            "f1": 0.345156,
            "f1_weighted": 0.680389,
            "ap": 0.998294,
            "ap_weighted": 0.998294
          },
          {
            "accuracy": 0.6375,
            "f1": 0.394499,
            "f1_weighted": 0.776166,
            "ap": 0.998592,
            "ap_weighted": 0.998592
          }
        ],
        "main_score": 0.66845,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 15.09409475326538,
  "kg_co2_emissions": null
}