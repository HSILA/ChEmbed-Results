{
  "dataset_revision": "7d8df44e588b6143d4856c781f72f919fa0599a7",
  "task_name": "WikipediaSolidStateColloidalClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.795495,
        "f1": 0.793209,
        "f1_weighted": 0.794488,
        "ap": 0.773822,
        "ap_weighted": 0.773822,
        "scores_per_experiment": [
          {
            "accuracy": 0.79955,
            "f1": 0.799321,
            "f1_weighted": 0.800023,
            "ap": 0.78791,
            "ap_weighted": 0.78791
          },
          {
            "accuracy": 0.671171,
            "f1": 0.66848,
            "f1_weighted": 0.665386,
            "ap": 0.688446,
            "ap_weighted": 0.688446
          },
          {
            "accuracy": 0.806306,
            "f1": 0.804877,
            "f1_weighted": 0.806607,
            "ap": 0.78103,
            "ap_weighted": 0.78103
          },
          {
            "accuracy": 0.831081,
            "f1": 0.826388,
            "f1_weighted": 0.829345,
            "ap": 0.786962,
            "ap_weighted": 0.786962
          },
          {
            "accuracy": 0.801802,
            "f1": 0.801605,
            "f1_weighted": 0.802253,
            "ap": 0.790975,
            "ap_weighted": 0.790975
          },
          {
            "accuracy": 0.824324,
            "f1": 0.821687,
            "f1_weighted": 0.823934,
            "ap": 0.790291,
            "ap_weighted": 0.790291
          },
          {
            "accuracy": 0.817568,
            "f1": 0.808815,
            "f1_weighted": 0.813053,
            "ap": 0.76408,
            "ap_weighted": 0.76408
          },
          {
            "accuracy": 0.855856,
            "f1": 0.854554,
            "f1_weighted": 0.85598,
            "ap": 0.830595,
            "ap_weighted": 0.830595
          },
          {
            "accuracy": 0.765766,
            "f1": 0.765647,
            "f1_weighted": 0.766194,
            "ap": 0.755744,
            "ap_weighted": 0.755744
          },
          {
            "accuracy": 0.781532,
            "f1": 0.780721,
            "f1_weighted": 0.782102,
            "ap": 0.762182,
            "ap_weighted": 0.762182
          }
        ],
        "main_score": 0.795495,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 1.5882558822631836,
  "kg_co2_emissions": null
}