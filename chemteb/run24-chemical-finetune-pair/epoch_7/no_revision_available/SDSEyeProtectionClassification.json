{
  "dataset_revision": "35cbe5ee544dd26e343238a333de4568e6f77819",
  "task_name": "SDSEyeProtectionClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.65595,
        "f1": 0.401254,
        "f1_weighted": 0.789083,
        "ap": 0.998588,
        "ap_weighted": 0.998588,
        "scores_per_experiment": [
          {
            "accuracy": 0.648,
            "f1": 0.398562,
            "f1_weighted": 0.783951,
            "ap": 0.998618,
            "ap_weighted": 0.998618
          },
          {
            "accuracy": 0.612,
            "f1": 0.383265,
            "f1_weighted": 0.756978,
            "ap": 0.998029,
            "ap_weighted": 0.998029
          },
          {
            "accuracy": 0.64,
            "f1": 0.39547,
            "f1_weighted": 0.778029,
            "ap": 0.998598,
            "ap_weighted": 0.998598
          },
          {
            "accuracy": 0.6355,
            "f1": 0.39499,
            "f1_weighted": 0.774542,
            "ap": 0.999086,
            "ap_weighted": 0.999086
          },
          {
            "accuracy": 0.6585,
            "f1": 0.401213,
            "f1_weighted": 0.791755,
            "ap": 0.998146,
            "ap_weighted": 0.998146
          },
          {
            "accuracy": 0.738,
            "f1": 0.431971,
            "f1_weighted": 0.84682,
            "ap": 0.998844,
            "ap_weighted": 0.998844
          },
          {
            "accuracy": 0.703,
            "f1": 0.420822,
            "f1_weighted": 0.823067,
            "ap": 0.999256,
            "ap_weighted": 0.999256
          },
          {
            "accuracy": 0.69,
            "f1": 0.414436,
            "f1_weighted": 0.814124,
            "ap": 0.998723,
            "ap_weighted": 0.998723
          },
          {
            "accuracy": 0.599,
            "f1": 0.378086,
            "f1_weighted": 0.746894,
            "ap": 0.997997,
            "ap_weighted": 0.997997
          },
          {
            "accuracy": 0.6355,
            "f1": 0.39372,
            "f1_weighted": 0.774672,
            "ap": 0.998587,
            "ap_weighted": 0.998587
          }
        ],
        "main_score": 0.65595,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 18.328611850738525,
  "kg_co2_emissions": null
}