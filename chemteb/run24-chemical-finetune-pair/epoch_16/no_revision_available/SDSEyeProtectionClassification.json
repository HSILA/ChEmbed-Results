{
  "dataset_revision": "35cbe5ee544dd26e343238a333de4568e6f77819",
  "task_name": "SDSEyeProtectionClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.63535,
        "f1": 0.393398,
        "f1_weighted": 0.773809,
        "ap": 0.998587,
        "ap_weighted": 0.998587,
        "scores_per_experiment": [
          {
            "accuracy": 0.633,
            "f1": 0.392745,
            "f1_weighted": 0.772799,
            "ap": 0.998581,
            "ap_weighted": 0.998581
          },
          {
            "accuracy": 0.578,
            "f1": 0.370641,
            "f1_weighted": 0.730087,
            "ap": 0.998443,
            "ap_weighted": 0.998443
          },
          {
            "accuracy": 0.627,
            "f1": 0.390394,
            "f1_weighted": 0.76828,
            "ap": 0.998566,
            "ap_weighted": 0.998566
          },
          {
            "accuracy": 0.616,
            "f1": 0.387246,
            "f1_weighted": 0.759767,
            "ap": 0.999038,
            "ap_weighted": 0.999038
          },
          {
            "accuracy": 0.643,
            "f1": 0.396632,
            "f1_weighted": 0.780256,
            "ap": 0.998606,
            "ap_weighted": 0.998606
          },
          {
            "accuracy": 0.7205,
            "f1": 0.425639,
            "f1_weighted": 0.835111,
            "ap": 0.9988,
            "ap_weighted": 0.9988
          },
          {
            "accuracy": 0.6885,
            "f1": 0.415383,
            "f1_weighted": 0.812971,
            "ap": 0.999219,
            "ap_weighted": 0.999219
          },
          {
            "accuracy": 0.65,
            "f1": 0.399331,
            "f1_weighted": 0.785423,
            "ap": 0.998623,
            "ap_weighted": 0.998623
          },
          {
            "accuracy": 0.599,
            "f1": 0.378086,
            "f1_weighted": 0.746894,
            "ap": 0.997997,
            "ap_weighted": 0.997997
          },
          {
            "accuracy": 0.5985,
            "f1": 0.377886,
            "f1_weighted": 0.746502,
            "ap": 0.997995,
            "ap_weighted": 0.997995
          }
        ],
        "main_score": 0.63535,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 13.974626302719116,
  "kg_co2_emissions": null
}