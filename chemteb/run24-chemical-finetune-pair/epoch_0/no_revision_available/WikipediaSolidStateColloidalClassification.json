{
  "dataset_revision": "7d8df44e588b6143d4856c781f72f919fa0599a7",
  "task_name": "WikipediaSolidStateColloidalClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.781532,
        "f1": 0.777958,
        "f1_weighted": 0.779355,
        "ap": 0.758689,
        "ap_weighted": 0.758689,
        "scores_per_experiment": [
          {
            "accuracy": 0.797297,
            "f1": 0.7949,
            "f1_weighted": 0.797197,
            "ap": 0.766719,
            "ap_weighted": 0.766719
          },
          {
            "accuracy": 0.617117,
            "f1": 0.609995,
            "f1_weighted": 0.604535,
            "ap": 0.647002,
            "ap_weighted": 0.647002
          },
          {
            "accuracy": 0.808559,
            "f1": 0.806912,
            "f1_weighted": 0.808759,
            "ap": 0.781623,
            "ap_weighted": 0.781623
          },
          {
            "accuracy": 0.813063,
            "f1": 0.802862,
            "f1_weighted": 0.807508,
            "ap": 0.757532,
            "ap_weighted": 0.757532
          },
          {
            "accuracy": 0.77027,
            "f1": 0.770266,
            "f1_weighted": 0.770158,
            "ap": 0.767697,
            "ap_weighted": 0.767697
          },
          {
            "accuracy": 0.792793,
            "f1": 0.790131,
            "f1_weighted": 0.79258,
            "ap": 0.761356,
            "ap_weighted": 0.761356
          },
          {
            "accuracy": 0.822072,
            "f1": 0.815629,
            "f1_weighted": 0.8192,
            "ap": 0.773538,
            "ap_weighted": 0.773538
          },
          {
            "accuracy": 0.837838,
            "f1": 0.836881,
            "f1_weighted": 0.838175,
            "ap": 0.816483,
            "ap_weighted": 0.816483
          },
          {
            "accuracy": 0.779279,
            "f1": 0.779261,
            "f1_weighted": 0.779467,
            "ap": 0.773421,
            "ap_weighted": 0.773421
          },
          {
            "accuracy": 0.777027,
            "f1": 0.772737,
            "f1_weighted": 0.775972,
            "ap": 0.741518,
            "ap_weighted": 0.741518
          }
        ],
        "main_score": 0.781532,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 2.230959177017212,
  "kg_co2_emissions": null
}