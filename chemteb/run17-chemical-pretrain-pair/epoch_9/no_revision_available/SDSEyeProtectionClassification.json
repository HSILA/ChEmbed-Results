{
  "dataset_revision": "35cbe5ee544dd26e343238a333de4568e6f77819",
  "task_name": "SDSEyeProtectionClassification",
  "mteb_version": "1.36.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.661,
        "f1": 0.401265,
        "f1_weighted": 0.790204,
        "ap": 0.998352,
        "ap_weighted": 0.998352,
        "scores_per_experiment": [
          {
            "accuracy": 0.753,
            "f1": 0.433485,
            "f1_weighted": 0.85681,
            "ap": 0.997884,
            "ap_weighted": 0.997884
          },
          {
            "accuracy": 0.632,
            "f1": 0.389819,
            "f1_weighted": 0.772311,
            "ap": 0.997581,
            "ap_weighted": 0.997581
          },
          {
            "accuracy": 0.641,
            "f1": 0.395858,
            "f1_weighted": 0.778772,
            "ap": 0.998601,
            "ap_weighted": 0.998601
          },
          {
            "accuracy": 0.579,
            "f1": 0.372133,
            "f1_weighted": 0.730727,
            "ap": 0.998945,
            "ap_weighted": 0.998945
          },
          {
            "accuracy": 0.752,
            "f1": 0.435078,
            "f1_weighted": 0.856089,
            "ap": 0.99838,
            "ap_weighted": 0.99838
          },
          {
            "accuracy": 0.7665,
            "f1": 0.440139,
            "f1_weighted": 0.865456,
            "ap": 0.998416,
            "ap_weighted": 0.998416
          },
          {
            "accuracy": 0.6325,
            "f1": 0.39255,
            "f1_weighted": 0.772423,
            "ap": 0.998579,
            "ap_weighted": 0.998579
          },
          {
            "accuracy": 0.7665,
            "f1": 0.44218,
            "f1_weighted": 0.865391,
            "ap": 0.998915,
            "ap_weighted": 0.998915
          },
          {
            "accuracy": 0.5495,
            "f1": 0.359648,
            "f1_weighted": 0.706576,
            "ap": 0.998871,
            "ap_weighted": 0.998871
          },
          {
            "accuracy": 0.538,
            "f1": 0.351765,
            "f1_weighted": 0.697481,
            "ap": 0.997346,
            "ap_weighted": 0.997346
          }
        ],
        "main_score": 0.661,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 12.412731885910034,
  "kg_co2_emissions": null
}