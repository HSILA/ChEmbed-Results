{
  "dataset_revision": "35cbe5ee544dd26e343238a333de4568e6f77819",
  "task_name": "SDSEyeProtectionClassification",
  "mteb_version": "1.36.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.65965,
        "f1": 0.400624,
        "f1_weighted": 0.789351,
        "ap": 0.998298,
        "ap_weighted": 0.998298,
        "scores_per_experiment": [
          {
            "accuracy": 0.756,
            "f1": 0.43451,
            "f1_weighted": 0.858757,
            "ap": 0.997891,
            "ap_weighted": 0.997891
          },
          {
            "accuracy": 0.5925,
            "f1": 0.376595,
            "f1_weighted": 0.741634,
            "ap": 0.998479,
            "ap_weighted": 0.998479
          },
          {
            "accuracy": 0.64,
            "f1": 0.39547,
            "f1_weighted": 0.778029,
            "ap": 0.998598,
            "ap_weighted": 0.998598
          },
          {
            "accuracy": 0.5885,
            "f1": 0.376069,
            "f1_weighted": 0.738312,
            "ap": 0.998969,
            "ap_weighted": 0.998969
          },
          {
            "accuracy": 0.7455,
            "f1": 0.432798,
            "f1_weighted": 0.85184,
            "ap": 0.998364,
            "ap_weighted": 0.998364
          },
          {
            "accuracy": 0.759,
            "f1": 0.437526,
            "f1_weighted": 0.86063,
            "ap": 0.998397,
            "ap_weighted": 0.998397
          },
          {
            "accuracy": 0.632,
            "f1": 0.39109,
            "f1_weighted": 0.77218,
            "ap": 0.998079,
            "ap_weighted": 0.998079
          },
          {
            "accuracy": 0.7755,
            "f1": 0.443265,
            "f1_weighted": 0.871192,
            "ap": 0.998439,
            "ap_weighted": 0.998439
          },
          {
            "accuracy": 0.5575,
            "f1": 0.362059,
            "f1_weighted": 0.713394,
            "ap": 0.998391,
            "ap_weighted": 0.998391
          },
          {
            "accuracy": 0.55,
            "f1": 0.356864,
            "f1_weighted": 0.70754,
            "ap": 0.997376,
            "ap_weighted": 0.997376
          }
        ],
        "main_score": 0.65965,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 10.976151704788208,
  "kg_co2_emissions": null
}