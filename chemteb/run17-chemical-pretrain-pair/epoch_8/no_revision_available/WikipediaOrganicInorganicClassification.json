{
  "dataset_revision": "96d1d9b37c4693f74c46c83d63a290573f78d511",
  "task_name": "WikipediaOrganicInorganicClassification",
  "mteb_version": "1.36.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.874905,
        "f1": 0.872297,
        "f1_weighted": 0.874197,
        "ap": 0.805182,
        "ap_weighted": 0.805182,
        "scores_per_experiment": [
          {
            "accuracy": 0.908745,
            "f1": 0.90518,
            "f1_weighted": 0.907487,
            "ap": 0.871959,
            "ap_weighted": 0.871959
          },
          {
            "accuracy": 0.878327,
            "f1": 0.873952,
            "f1_weighted": 0.876898,
            "ap": 0.818615,
            "ap_weighted": 0.818615
          },
          {
            "accuracy": 0.893536,
            "f1": 0.887483,
            "f1_weighted": 0.890758,
            "ap": 0.862986,
            "ap_weighted": 0.862986
          },
          {
            "accuracy": 0.809886,
            "f1": 0.809883,
            "f1_weighted": 0.809792,
            "ap": 0.698583,
            "ap_weighted": 0.698583
          },
          {
            "accuracy": 0.882129,
            "f1": 0.876104,
            "f1_weighted": 0.879532,
            "ap": 0.837768,
            "ap_weighted": 0.837768
          },
          {
            "accuracy": 0.863118,
            "f1": 0.862063,
            "f1_weighted": 0.863577,
            "ap": 0.773186,
            "ap_weighted": 0.773186
          },
          {
            "accuracy": 0.889734,
            "f1": 0.886559,
            "f1_weighted": 0.88894,
            "ap": 0.830644,
            "ap_weighted": 0.830644
          },
          {
            "accuracy": 0.885932,
            "f1": 0.8852,
            "f1_weighted": 0.88635,
            "ap": 0.803546,
            "ap_weighted": 0.803546
          },
          {
            "accuracy": 0.86692,
            "f1": 0.866146,
            "f1_weighted": 0.867423,
            "ap": 0.776608,
            "ap_weighted": 0.776608
          },
          {
            "accuracy": 0.870722,
            "f1": 0.870406,
            "f1_weighted": 0.87121,
            "ap": 0.77792,
            "ap_weighted": 0.77792
          }
        ],
        "main_score": 0.874905,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 1.121387243270874,
  "kg_co2_emissions": null
}