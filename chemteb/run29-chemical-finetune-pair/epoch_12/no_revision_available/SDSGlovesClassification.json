{
  "dataset_revision": "c723236c5ec417d79512e6104aca9d2cd88168f6",
  "task_name": "SDSGlovesClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.70725,
        "f1": 0.419132,
        "f1_weighted": 0.822393,
        "ap": 0.996781,
        "ap_weighted": 0.996781,
        "scores_per_experiment": [
          {
            "accuracy": 0.7805,
            "f1": 0.440593,
            "f1_weighted": 0.873162,
            "ap": 0.995634,
            "ap_weighted": 0.995634
          },
          {
            "accuracy": 0.8135,
            "f1": 0.451218,
            "f1_weighted": 0.893536,
            "ap": 0.995766,
            "ap_weighted": 0.995766
          },
          {
            "accuracy": 0.647,
            "f1": 0.398177,
            "f1_weighted": 0.782053,
            "ap": 0.996589,
            "ap_weighted": 0.996589
          },
          {
            "accuracy": 0.7135,
            "f1": 0.426362,
            "f1_weighted": 0.828964,
            "ap": 0.997851,
            "ap_weighted": 0.997851
          },
          {
            "accuracy": 0.844,
            "f1": 0.463984,
            "f1_weighted": 0.911699,
            "ap": 0.996384,
            "ap_weighted": 0.996384
          },
          {
            "accuracy": 0.677,
            "f1": 0.411031,
            "f1_weighted": 0.803652,
            "ap": 0.997206,
            "ap_weighted": 0.997206
          },
          {
            "accuracy": 0.668,
            "f1": 0.408996,
            "f1_weighted": 0.797111,
            "ap": 0.997668,
            "ap_weighted": 0.997668
          },
          {
            "accuracy": 0.636,
            "f1": 0.395187,
            "f1_weighted": 0.773771,
            "ap": 0.997042,
            "ap_weighted": 0.997042
          },
          {
            "accuracy": 0.5865,
            "f1": 0.375243,
            "f1_weighted": 0.735633,
            "ap": 0.996843,
            "ap_weighted": 0.996843
          },
          {
            "accuracy": 0.7065,
            "f1": 0.420526,
            "f1_weighted": 0.82435,
            "ap": 0.996827,
            "ap_weighted": 0.996827
          }
        ],
        "main_score": 0.70725,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 15.767122507095337,
  "kg_co2_emissions": null
}