{
  "dataset_revision": "35cbe5ee544dd26e343238a333de4568e6f77819",
  "task_name": "SDSEyeProtectionClassification",
  "mteb_version": "1.36.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.689,
        "f1": 0.411443,
        "f1_weighted": 0.811393,
        "ap": 0.998172,
        "ap_weighted": 0.998172,
        "scores_per_experiment": [
          {
            "accuracy": 0.717,
            "f1": 0.421001,
            "f1_weighted": 0.832916,
            "ap": 0.997794,
            "ap_weighted": 0.997794
          },
          {
            "accuracy": 0.663,
            "f1": 0.404298,
            "f1_weighted": 0.794903,
            "ap": 0.998656,
            "ap_weighted": 0.998656
          },
          {
            "accuracy": 0.647,
            "f1": 0.396853,
            "f1_weighted": 0.783338,
            "ap": 0.998117,
            "ap_weighted": 0.998117
          },
          {
            "accuracy": 0.6145,
            "f1": 0.385451,
            "f1_weighted": 0.758757,
            "ap": 0.998534,
            "ap_weighted": 0.998534
          },
          {
            "accuracy": 0.773,
            "f1": 0.440279,
            "f1_weighted": 0.869666,
            "ap": 0.997934,
            "ap_weighted": 0.997934
          },
          {
            "accuracy": 0.7775,
            "f1": 0.441796,
            "f1_weighted": 0.872519,
            "ap": 0.997945,
            "ap_weighted": 0.997945
          },
          {
            "accuracy": 0.7285,
            "f1": 0.425029,
            "f1_weighted": 0.840657,
            "ap": 0.997823,
            "ap_weighted": 0.997823
          },
          {
            "accuracy": 0.7605,
            "f1": 0.440038,
            "f1_weighted": 0.861531,
            "ap": 0.9989,
            "ap_weighted": 0.9989
          },
          {
            "accuracy": 0.658,
            "f1": 0.402395,
            "f1_weighted": 0.791274,
            "ap": 0.998643,
            "ap_weighted": 0.998643
          },
          {
            "accuracy": 0.551,
            "f1": 0.357285,
            "f1_weighted": 0.708371,
            "ap": 0.997379,
            "ap_weighted": 0.997379
          }
        ],
        "main_score": 0.689,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 10.901273250579834,
  "kg_co2_emissions": null
}