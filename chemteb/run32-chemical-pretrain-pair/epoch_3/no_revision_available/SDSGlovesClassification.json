{
  "dataset_revision": "c723236c5ec417d79512e6104aca9d2cd88168f6",
  "task_name": "SDSGlovesClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.57835,
        "f1": 0.361965,
        "f1_weighted": 0.708105,
        "ap": 0.996911,
        "ap_weighted": 0.996911,
        "scores_per_experiment": [
          {
            "accuracy": 0.746,
            "f1": 0.434847,
            "f1_weighted": 0.850836,
            "ap": 0.996985,
            "ap_weighted": 0.996985
          },
          {
            "accuracy": 0.7875,
            "f1": 0.449664,
            "f1_weighted": 0.877403,
            "ap": 0.997152,
            "ap_weighted": 0.997152
          },
          {
            "accuracy": 0.3415,
            "f1": 0.258538,
            "f1_weighted": 0.504573,
            "ap": 0.996856,
            "ap_weighted": 0.996856
          },
          {
            "accuracy": 0.6405,
            "f1": 0.396953,
            "f1_weighted": 0.777124,
            "ap": 0.99706,
            "ap_weighted": 0.99706
          },
          {
            "accuracy": 0.7815,
            "f1": 0.443141,
            "f1_weighted": 0.87374,
            "ap": 0.996134,
            "ap_weighted": 0.996134
          },
          {
            "accuracy": 0.7085,
            "f1": 0.421259,
            "f1_weighted": 0.82572,
            "ap": 0.996835,
            "ap_weighted": 0.996835
          },
          {
            "accuracy": 0.562,
            "f1": 0.366011,
            "f1_weighted": 0.715689,
            "ap": 0.997243,
            "ap_weighted": 0.997243
          },
          {
            "accuracy": 0.2125,
            "f1": 0.177423,
            "f1_weighted": 0.345927,
            "ap": 0.995842,
            "ap_weighted": 0.995842
          },
          {
            "accuracy": 0.3795,
            "f1": 0.280152,
            "f1_weighted": 0.545436,
            "ap": 0.997508,
            "ap_weighted": 0.997508
          },
          {
            "accuracy": 0.624,
            "f1": 0.391661,
            "f1_weighted": 0.764607,
            "ap": 0.997492,
            "ap_weighted": 0.997492
          }
        ],
        "main_score": 0.57835,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 15.658000469207764,
  "kg_co2_emissions": null
}