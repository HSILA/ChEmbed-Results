{
  "dataset_revision": "35cbe5ee544dd26e343238a333de4568e6f77819",
  "task_name": "SDSEyeProtectionClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.67965,
        "f1": 0.408043,
        "f1_weighted": 0.804857,
        "ap": 0.998149,
        "ap_weighted": 0.998149,
        "scores_per_experiment": [
          {
            "accuracy": 0.743,
            "f1": 0.430053,
            "f1_weighted": 0.850272,
            "ap": 0.997859,
            "ap_weighted": 0.997859
          },
          {
            "accuracy": 0.6895,
            "f1": 0.41273,
            "f1_weighted": 0.813875,
            "ap": 0.998223,
            "ap_weighted": 0.998223
          },
          {
            "accuracy": 0.642,
            "f1": 0.394942,
            "f1_weighted": 0.779641,
            "ap": 0.998104,
            "ap_weighted": 0.998104
          },
          {
            "accuracy": 0.648,
            "f1": 0.397234,
            "f1_weighted": 0.784074,
            "ap": 0.998119,
            "ap_weighted": 0.998119
          },
          {
            "accuracy": 0.754,
            "f1": 0.433827,
            "f1_weighted": 0.85746,
            "ap": 0.997886,
            "ap_weighted": 0.997886
          },
          {
            "accuracy": 0.701,
            "f1": 0.415329,
            "f1_weighted": 0.821971,
            "ap": 0.997754,
            "ap_weighted": 0.997754
          },
          {
            "accuracy": 0.699,
            "f1": 0.417767,
            "f1_weighted": 0.820396,
            "ap": 0.998746,
            "ap_weighted": 0.998746
          },
          {
            "accuracy": 0.744,
            "f1": 0.432271,
            "f1_weighted": 0.850855,
            "ap": 0.99836,
            "ap_weighted": 0.99836
          },
          {
            "accuracy": 0.668,
            "f1": 0.407599,
            "f1_weighted": 0.798397,
            "ap": 0.999168,
            "ap_weighted": 0.999168
          },
          {
            "accuracy": 0.508,
            "f1": 0.338678,
            "f1_weighted": 0.671634,
            "ap": 0.997271,
            "ap_weighted": 0.997271
          }
        ],
        "main_score": 0.67965,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 16.85172152519226,
  "kg_co2_emissions": null
}