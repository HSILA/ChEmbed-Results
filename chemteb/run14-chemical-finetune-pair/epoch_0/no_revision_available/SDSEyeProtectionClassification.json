{
  "dataset_revision": "35cbe5ee544dd26e343238a333de4568e6f77819",
  "task_name": "SDSEyeProtectionClassification",
  "mteb_version": "1.36.29",
  "scores": {
    "test": [
      {
        "accuracy": 0.6624,
        "f1": 0.402755,
        "f1_weighted": 0.791904,
        "ap": 0.998555,
        "ap_weighted": 0.998555,
        "scores_per_experiment": [
          {
            "accuracy": 0.6785,
            "f1": 0.408681,
            "f1_weighted": 0.80612,
            "ap": 0.998196,
            "ap_weighted": 0.998196
          },
          {
            "accuracy": 0.605,
            "f1": 0.381652,
            "f1_weighted": 0.751421,
            "ap": 0.99851,
            "ap_weighted": 0.99851
          },
          {
            "accuracy": 0.5575,
            "f1": 0.362059,
            "f1_weighted": 0.713394,
            "ap": 0.998391,
            "ap_weighted": 0.998391
          },
          {
            "accuracy": 0.651,
            "f1": 0.399716,
            "f1_weighted": 0.786158,
            "ap": 0.998626,
            "ap_weighted": 0.998626
          },
          {
            "accuracy": 0.66,
            "f1": 0.404527,
            "f1_weighted": 0.792612,
            "ap": 0.999148,
            "ap_weighted": 0.999148
          },
          {
            "accuracy": 0.786,
            "f1": 0.446901,
            "f1_weighted": 0.877812,
            "ap": 0.998465,
            "ap_weighted": 0.998465
          },
          {
            "accuracy": 0.6955,
            "f1": 0.414922,
            "f1_weighted": 0.818063,
            "ap": 0.998238,
            "ap_weighted": 0.998238
          },
          {
            "accuracy": 0.792,
            "f1": 0.451268,
            "f1_weighted": 0.881507,
            "ap": 0.998979,
            "ap_weighted": 0.998979
          },
          {
            "accuracy": 0.555,
            "f1": 0.360999,
            "f1_weighted": 0.711328,
            "ap": 0.998385,
            "ap_weighted": 0.998385
          },
          {
            "accuracy": 0.6435,
            "f1": 0.396826,
            "f1_weighted": 0.780627,
            "ap": 0.998607,
            "ap_weighted": 0.998607
          }
        ],
        "main_score": 0.6624,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 10.93607473373413,
  "kg_co2_emissions": null
}